<!-- 1b139717-ed05-47c0-bb5c-571ecea8f7b8 4c9e1108-982e-4eb7-acd0-5f7c131b9418 -->
# AI-Assisted IRB Review System

## Overview

Add automated ethical review capabilities to SONA using multi-agent AI analysis. Researchers can upload materials or link OSF repos, trigger AI review, address flagged issues, and generate reports for IRB committees. Full audit trail maintained.

## Database Models

### New Model: `IRBReview` (`apps/studies/models.py`)

```python
class IRBReview(models.Model):
    REVIEW_STATUS = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed')
    ]
    
    study = models.ForeignKey(Study, on_delete=CASCADE, related_name='irb_reviews')
    version = models.IntegerField(default=1)  # Track protocol versions
    initiated_by = models.ForeignKey(User, on_delete=SET_NULL, null=True)
    initiated_at = models.DateTimeField(auto_now_add=True)
    completed_at = models.DateTimeField(null=True, blank=True)
    status = models.CharField(max_length=20, choices=REVIEW_STATUS, default='pending')
    
    # Material sources
    osf_repo_url = models.URLField(blank=True)
    uploaded_files = models.JSONField(default=list)  # List of file metadata
    
    # AI Analysis results
    ethics_analysis = models.JSONField(null=True)      # Ethics agent findings
    privacy_analysis = models.JSONField(null=True)     # Privacy agent findings
    vulnerability_analysis = models.JSONField(null=True)  # Vulnerable populations
    data_security_analysis = models.JSONField(null=True)  # Data handling
    consent_analysis = models.JSONField(null=True)     # Consent adequacy
    
    # Aggregated results
    overall_risk_level = models.CharField(max_length=20)  # minimal, low, moderate, high
    critical_issues = models.JSONField(default=list)
    moderate_issues = models.JSONField(default=list)
    minor_issues = models.JSONField(default=list)
    recommendations = models.JSONField(default=list)
    
    # Researcher response
    researcher_notes = models.TextField(blank=True)
    issues_addressed = models.JSONField(default=list)  # Which issues were fixed
    
    # Audit
    ai_model_versions = models.JSONField(default=dict)  # Track which AI models used
    processing_time_seconds = models.IntegerField(null=True)
```

### Model: `ReviewDocument` (uploaded files)

```python
class ReviewDocument(models.Model):
    review = models.ForeignKey(IRBReview, on_delete=CASCADE, related_name='documents')
    file = models.FileField(upload_to='irb_reviews/%Y/%m/')
    filename = models.CharField(max_length=255)
    file_type = models.CharField(max_length=50)  # protocol, consent, survey, etc.
    uploaded_at = models.DateTimeField(auto_now_add=True)
    file_hash = models.CharField(max_length=64)  # SHA256 for integrity
```

## AI Review Engine

### Core Analysis Module: `apps/studies/irb_ai/analyzer.py`

```python
class IRBAnalyzer:
    """Orchestrates multi-agent IRB review"""
    
    def __init__(self, review_id):
        self.review = IRBReview.objects.get(id=review_id)
        self.agents = {
            'ethics': EthicsAgent(),
            'privacy': PrivacyAgent(),
            'vulnerability': VulnerabilityAgent(),
            'data_security': DataSecurityAgent(),
            'consent': ConsentAgent()
        }
    
    async def run_review(self):
        """Execute all agents in parallel, aggregate results"""
        # 1. Extract materials (from uploads or OSF)
        materials = await self.gather_materials()
        
        # 2. Run agents concurrently
        results = await asyncio.gather(*[
            agent.analyze(materials) for agent in self.agents.values()
        ])
        
        # 3. Aggregate and categorize issues
        self.categorize_findings(results)
        
        # 4. Generate recommendations
        self.generate_recommendations()
        
        # 5. Save to database
        self.save_results()
```

### Individual Agents: `apps/studies/irb_ai/agents/`

Each agent (ethics, privacy, vulnerability, data_security, consent) implements:

```python
class BaseAgent:
    def __init__(self):
        self.criteria = self.load_criteria()  # From IRB_Automation_Toolkit
    
    async def analyze(self, materials: dict) -> dict:
        """
        Use Claude API with structured prompts:
        - Load Nicholls IRB criteria from IRB_Automation_Toolkit
        - Analyze protocols, consent forms, procedures
        - Return structured findings with severity levels
        """
        prompt = self.build_prompt(materials)
        response = await self.call_ai_api(prompt)
        return self.parse_findings(response)
    
    def load_criteria(self):
        """Load from IRB_Automation_Toolkit/configs/nicholls_hsirb_settings.json"""
```

### OSF Integration: `apps/studies/irb_ai/osf_client.py`

```python
class OSFClient:
    def fetch_repo_files(self, osf_url: str) -> list:
        """
        Fetch file list and download relevant files:
        - Protocol documents
        - Consent forms
        - Survey instruments
        Return as structured dict for AI analysis
        """
```

## Researcher UI

### New View: `irb_review_create` (`apps/studies/views.py`)

Upload interface or OSF link input. If OSF link exists on study, pre-fill and offer one-click import.

### New View: `irb_review_detail`

Display review results with:

- Overall risk assessment (color-coded)
- Categorized issues (critical/moderate/minor)
- Each issue with: description, recommendation, severity, affected section
- Form to record how issues were addressed
- Button to trigger re-review after changes

### New View: `irb_review_history`

List all reviews for a study with version numbers, timestamps, risk levels. Compare versions.

## Committee Access

### Admin Interface Updates: `apps/studies/admin.py`

```python
class IRBReviewAdmin(admin.ModelAdmin):
    list_display = ['study', 'version', 'status', 'overall_risk_level', 'initiated_at']
    list_filter = ['status', 'overall_risk_level', 'initiated_at']
    readonly_fields = ['all analysis fields', 'ai_model_versions']
    
    # Allow committee to trigger reviews
    actions = ['trigger_committee_review']
```

### Committee Dashboard: `templates/studies/committee_review.html`

Filter studies by IRB status, view AI reviews alongside submissions, generate summary reports for review packets.

## Background Processing

### Celery Task: `apps/studies/tasks.py`

```python
@shared_task
def run_irb_ai_review(review_id):
    """
    Background task to run AI analysis:
    1. Update status to 'in_progress'
    2. Initialize IRBAnalyzer
    3. Run analysis
    4. Send email notification when complete
    """
```

## Templates

### `templates/studies/irb_review_create.html`

- File upload interface (drag-drop)
- OSF link input (if study.osf_enabled, show current link)
- File type categorization (protocol, consent, survey, other)
- Submit for AI review button

### `templates/studies/irb_review_report.html`

- Risk level badge (minimal/low/moderate/high)
- Expandable sections for each agent's findings
- Issue cards with severity, description, recommendation, affected section
- Researcher response form
- Download report as PDF button
- Version comparison view

### `templates/studies/committee_dashboard.html`

- Study list filtered by IRB status
- AI review summary cards
- Bulk operations (approve, request changes, etc.)

## Integration Points

### Study Model Updates: `apps/studies/models.py`

```python
class Study(models.Model):
    # Add property
    @property
    def latest_irb_review(self):
        return self.irb_reviews.order_by('-version').first()
    
    @property
    def irb_review_status(self):
        """Has review been run? Any critical issues?"""
```

### Researcher Dashboard: `templates/studies/researcher_dashboard.html`

Add "AI IRB Review" button next to each study. Badge showing latest review status (pending, issues, clear).

### IRB Automation Toolkit Integration

Load criteria from `IRB_Automation_Toolkit/configs/nicholls_hsirb_settings.json`. Use templates from `IRB_Automation_Toolkit/templates/` for report generation.

## Audit Trail

All fields in `IRBReview` model are immutable once completed. New reviews create new version records. Track:

- Who initiated (researcher vs committee)
- All uploaded files with SHA256 hashes
- AI model versions used
- Processing time
- Researcher responses to each issue
- Timestamp chain

## API Endpoints

### `POST /api/studies/<uuid>/irb-review/create/`

Accepts files or OSF URL, creates IRBReview, triggers background task.

### `GET /api/studies/<uuid>/irb-review/<int:version>/`

Returns full review results as JSON.

### `POST /api/studies/<uuid>/irb-review/<int:version>/respond/`

Submit researcher responses to flagged issues.

## Migration Strategy

1. Create IRBReview and ReviewDocument models
2. Add media storage for uploaded files
3. Create `apps/studies/irb_ai/` package structure
4. Implement base analyzer and one agent (ethics) first
5. Add UI views incrementally (create → report → history)
6. Integrate with existing IRB fields on Study model
7. Add committee dashboard last

## Dependencies

- Add to `requirements.txt`: `anthropic` (Claude API), `aiohttp`, `PyPDF2` (for document parsing)
- Optional: `osf` Python client if available
- Celery already configured

## Configuration

Add to `config/settings.py`:

```python
# AI IRB Review
ANTHROPIC_API_KEY = config('ANTHROPIC_API_KEY', default='')
IRB_AI_MODEL = config('IRB_AI_MODEL', default='claude-3-5-sonnet-20241022')
IRB_REVIEW_STORAGE = 'media/irb_reviews/'
```

## Testing Strategy

- Unit tests for each agent with sample protocols
- Integration test for full review workflow
- Test OSF integration with mock repo
- Test version tracking and comparison
- Verify audit trail completeness

### To-dos

- [ ] Create IRBReview and ReviewDocument models with migrations
- [ ] Set up apps/studies/irb_ai/ package with analyzer.py and agents/
- [ ] Implement first agent (EthicsAgent) with Claude API integration
- [ ] Implement remaining agents (privacy, vulnerability, data_security, consent)
- [ ] Create OSF client for fetching repo files
- [ ] Add Celery task for background AI review processing
- [ ] Build researcher UI (create review, view report, respond to issues)
- [ ] Build committee dashboard for reviewing AI reports
- [ ] Add IRBReview to Django admin with appropriate permissions
- [ ] Implement version tracking and audit logging
- [ ] Load criteria from IRB_Automation_Toolkit configs
- [ ] Write tests for agents, workflow, and audit trail